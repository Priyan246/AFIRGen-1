# CloudWatch Dashboards Implementation Guide

## Overview

This guide covers the implementation of CloudWatch dashboards and metrics for AFIRGen, providing comprehensive monitoring and observability for the application.

## Components Implemented

### 1. CloudWatch Metrics Module (`cloudwatch_metrics.py`)

A Python module that publishes custom application metrics to AWS CloudWatch.

**Key Features:**
- Automatic metric buffering and batching (20 metrics per request)
- Async support for non-blocking operations
- Auto-detection of environment (disabled in local development)
- Comprehensive metric types: count, duration, percentage, size
- Decorator support for tracking function execution time
- Convenience functions for common metrics

**Metrics Published:**
- `APIRequests` - Total API requests by endpoint, method, status code
- `APILatency` - API response time in milliseconds
- `APIErrors` - API errors by endpoint and status code
- `FIRGenerations` - FIR generation attempts (success/failure)
- `FIRGenerationDuration` - Time to generate FIR by step
- `ModelInferences` - Model inference calls by model type
- `ModelInferenceDuration` - Model inference time
- `TokensGenerated` - Tokens generated by model
- `DatabaseOperations` - Database operations by type and status
- `DatabaseLatency` - Database query time
- `CacheOperations` - Cache hits and misses
- `RateLimitEvents` - Rate limiting events (blocked/allowed)
- `AuthenticationEvents` - Authentication attempts (success/failure)
- `HealthChecks` - Health check results by service

### 2. CloudWatch Dashboards (Terraform)

Three comprehensive dashboards for monitoring different aspects of the application:

#### Main Dashboard (`afirgen_main`)
- API performance (requests, errors, latency)
- FIR generation metrics
- Model inference activity
- Database performance
- Cache performance
- Security metrics (rate limiting, authentication)
- Health check status

#### Error Dashboard (`afirgen_errors`)
- Real-time error rate monitoring
- Errors by status code breakdown
- Recent error logs from CloudWatch Logs

#### Performance Dashboard (`afirgen_performance`)
- API latency by endpoint
- FIR generation duration by step
- Model inference duration by type
- Cache hit rate calculation
- Database latency by operation

### 3. CloudWatch Alarms (Terraform)

Comprehensive alerting for critical metrics:

**Critical Alarms:**
- High error rate (>5%)
- Database connection failures
- Service unhealthy status
- Composite alarm for critical system health

**High Priority Alarms:**
- FIR generation failures
- Authentication failures
- High API latency (P95 > 30s)

**Medium Priority Alarms:**
- High rate limiting activity
- Slow model inference
- Low cache hit rate (<50%)

**SNS Integration:**
- Email notifications for all alarms
- Configurable alarm email address

### 4. Application Integration

Metrics are automatically recorded throughout the application:

**Request Tracking Middleware:**
- Records all API requests with duration and status
- Tracks errors automatically

**Rate Limiting Middleware:**
- Records blocked and allowed requests
- Tracks rate limiting patterns

**Authentication Middleware:**
- Records successful and failed authentication attempts
- Tracks authentication failure reasons

**Health Check Endpoint:**
- Records health check results
- Tracks health check duration

**Graceful Shutdown:**
- Flushes all buffered metrics before shutdown
- Ensures no metric loss

## Configuration

### Environment Variables

```bash
# CloudWatch Configuration
AWS_REGION=us-east-1                    # AWS region for CloudWatch
ENVIRONMENT=production                   # Environment (development/staging/production)

# CloudWatch is automatically enabled in production and disabled in development
```

### Terraform Variables

```hcl
# terraform.tfvars
environment = "production"
aws_region  = "us-east-1"
alarm_email = "ops@example.com"  # Email for alarm notifications
```

## Deployment

### 1. Deploy CloudWatch Infrastructure

```bash
cd terraform

# Initialize Terraform
terraform init

# Review changes
terraform plan -var="alarm_email=ops@example.com"

# Deploy dashboards and alarms
terraform apply -var="alarm_email=ops@example.com"
```

### 2. Verify Deployment

```bash
# List dashboards
aws cloudwatch list-dashboards --region us-east-1

# List alarms
aws cloudwatch describe-alarms --region us-east-1

# Confirm SNS subscription
# Check email for subscription confirmation
```

### 3. Deploy Application

The application automatically starts publishing metrics when deployed to production:

```bash
# Build and deploy Docker containers
docker-compose up -d

# Verify metrics are being published
aws cloudwatch list-metrics --namespace AFIRGen --region us-east-1
```

## Viewing Dashboards

### AWS Console

1. Navigate to CloudWatch in AWS Console
2. Select "Dashboards" from the left menu
3. Open one of the AFIRGen dashboards:
   - `production-afirgen-main-dashboard`
   - `production-afirgen-errors-dashboard`
   - `production-afirgen-performance-dashboard`

### AWS CLI

```bash
# Get dashboard definition
aws cloudwatch get-dashboard \
  --dashboard-name production-afirgen-main-dashboard \
  --region us-east-1

# Get dashboard image (requires additional setup)
aws cloudwatch get-metric-widget-image \
  --metric-widget file://widget.json \
  --region us-east-1
```

## Monitoring Alarms

### View Alarm Status

```bash
# List all alarms
aws cloudwatch describe-alarms \
  --alarm-name-prefix production-afirgen \
  --region us-east-1

# Get alarm history
aws cloudwatch describe-alarm-history \
  --alarm-name production-afirgen-high-error-rate \
  --region us-east-1
```

### Test Alarms

```bash
# Set alarm to ALARM state for testing
aws cloudwatch set-alarm-state \
  --alarm-name production-afirgen-high-error-rate \
  --state-value ALARM \
  --state-reason "Testing alarm notification" \
  --region us-east-1
```

## Querying Metrics

### AWS CLI

```bash
# Get API request count
aws cloudwatch get-metric-statistics \
  --namespace AFIRGen \
  --metric-name APIRequests \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-01T23:59:59Z \
  --period 3600 \
  --statistics Sum \
  --region us-east-1

# Get API latency percentiles
aws cloudwatch get-metric-statistics \
  --namespace AFIRGen \
  --metric-name APILatency \
  --start-time 2024-01-01T00:00:00Z \
  --end-time 2024-01-01T23:59:59Z \
  --period 300 \
  --extended-statistics p95,p99 \
  --region us-east-1
```

### Python SDK

```python
import boto3
from datetime import datetime, timedelta

cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')

# Get metrics for last hour
response = cloudwatch.get_metric_statistics(
    Namespace='AFIRGen',
    MetricName='APIRequests',
    StartTime=datetime.utcnow() - timedelta(hours=1),
    EndTime=datetime.utcnow(),
    Period=300,
    Statistics=['Sum']
)

print(response['Datapoints'])
```

## Cost Optimization

### Metric Costs

CloudWatch pricing (as of 2024):
- First 10 custom metrics: Free
- Additional metrics: $0.30/metric/month
- API requests: $0.01/1000 requests
- Dashboard: $3/month

**AFIRGen Metrics:**
- ~15 unique metric names
- With dimensions, ~50-100 unique metric streams
- Estimated cost: $15-30/month

### Optimization Strategies

1. **Metric Buffering**: Metrics are buffered and sent in batches of 20
2. **Selective Publishing**: Only publish in production environment
3. **Dimension Optimization**: Use minimal dimensions to reduce metric streams
4. **Dashboard Consolidation**: Three dashboards instead of many small ones

### Monitoring Costs

```bash
# Get CloudWatch costs from Cost Explorer
aws ce get-cost-and-usage \
  --time-period Start=2024-01-01,End=2024-01-31 \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --filter file://cloudwatch-filter.json
```

## Troubleshooting

### Metrics Not Appearing

1. **Check IAM Permissions:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "cloudwatch:PutMetricData"
      ],
      "Resource": "*"
    }
  ]
}
```

2. **Verify Environment:**
```bash
# Check if CloudWatch is enabled
docker exec afirgen-main-backend python -c "
from cloudwatch_metrics import get_metrics
print(f'CloudWatch enabled: {get_metrics().enabled}')
"
```

3. **Check Logs:**
```bash
# View application logs
docker logs afirgen-main-backend | grep -i cloudwatch

# Check for boto3 errors
docker logs afirgen-main-backend | grep -i "boto3\|cloudwatch"
```

### Dashboard Not Loading

1. **Verify Dashboard Exists:**
```bash
aws cloudwatch get-dashboard \
  --dashboard-name production-afirgen-main-dashboard \
  --region us-east-1
```

2. **Check Metric Data:**
```bash
# Verify metrics are being published
aws cloudwatch list-metrics \
  --namespace AFIRGen \
  --region us-east-1
```

### Alarms Not Triggering

1. **Check Alarm Configuration:**
```bash
aws cloudwatch describe-alarms \
  --alarm-names production-afirgen-high-error-rate \
  --region us-east-1
```

2. **Verify SNS Subscription:**
```bash
aws sns list-subscriptions-by-topic \
  --topic-arn arn:aws:sns:us-east-1:ACCOUNT_ID:production-afirgen-cloudwatch-alarms
```

3. **Test Alarm Manually:**
```bash
aws cloudwatch set-alarm-state \
  --alarm-name production-afirgen-high-error-rate \
  --state-value ALARM \
  --state-reason "Manual test"
```

## Best Practices

### 1. Metric Naming

- Use consistent naming conventions
- Include service name in dimensions
- Use descriptive metric names

### 2. Dimension Strategy

- Keep dimensions minimal (2-3 per metric)
- Use high-cardinality values sparingly
- Consider cost impact of dimensions

### 3. Dashboard Design

- Group related metrics together
- Use appropriate time ranges
- Include both absolute and rate metrics
- Add annotations for deployments

### 4. Alarm Configuration

- Set appropriate thresholds based on baseline
- Use composite alarms for complex conditions
- Configure SNS topics for different severity levels
- Test alarms regularly

### 5. Cost Management

- Monitor CloudWatch costs monthly
- Review and remove unused metrics
- Optimize metric publishing frequency
- Use metric filters for log-based metrics

## Integration with Other Services

### CloudWatch Logs Insights

Query application logs alongside metrics:

```sql
fields @timestamp, level, message, duration_ms
| filter level = "ERROR"
| stats count() by bin(5m)
```

### X-Ray Integration

Add distributed tracing for deeper insights:

```python
from aws_xray_sdk.core import xray_recorder
from aws_xray_sdk.ext.fastapi.middleware import XRayMiddleware

app.add_middleware(XRayMiddleware, recorder=xray_recorder)
```

### CloudWatch Synthetics

Create canary tests for continuous monitoring:

```python
# canary.py
import requests

def handler(event, context):
    response = requests.get('https://api.afirgen.com/health')
    assert response.status_code == 200
    assert response.json()['status'] == 'healthy'
```

## Maintenance

### Regular Tasks

1. **Weekly:**
   - Review dashboard metrics
   - Check alarm history
   - Verify metric publishing

2. **Monthly:**
   - Review CloudWatch costs
   - Update alarm thresholds based on trends
   - Archive old dashboard versions

3. **Quarterly:**
   - Audit metric usage
   - Remove unused metrics
   - Update documentation

### Updating Dashboards

```bash
# Update dashboard configuration
cd terraform
terraform plan
terraform apply

# Or update via AWS CLI
aws cloudwatch put-dashboard \
  --dashboard-name production-afirgen-main-dashboard \
  --dashboard-body file://dashboard.json
```

## References

- [AWS CloudWatch Documentation](https://docs.aws.amazon.com/cloudwatch/)
- [CloudWatch Metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html)
- [CloudWatch Dashboards](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Dashboards.html)
- [CloudWatch Alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)
- [Boto3 CloudWatch](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudwatch.html)
